<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Bei Li</title>
  
  <meta name="author" content="Bei Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <!-- <p>[<a href="index_cn.html">‰∏≠Êñá‰∏ªÈ°µ</a>]</p>  -->
                <name>Bei Li</name>  
                
              </p>
              <p>I am a third year Ph.D student at the Department of <a href="http://www.cse.neu.edu.cn/">Computer Science and Technology</a> at <a href="http://www.neu.edu.cn/"> Northeastern University</a>, China. where I work at <a href="https://www.nlplab.com/">Natural Language Processing Lab </a> under the supervision of Prof. <a href="https://www.nlplab.com/members/xiaotong.html">Tong Xiao</a> and Prof. <a href="https://www.nlplab.com/members/zhujingbo.html"> Jingbo Zhu</a>.
              </p>
              <p>
                I received my bachelor degree in 2017 from Northeastern University, majoring in Computer Science and Technology, and my master in 2020 from Northeastern University, majoring in Computer Software and Theory.     
              </p>
              <p>
                I join the NEUNLP LAB at the fourth year of my college life. 
              </p>
              <p style="text-align:center">
                <a href="libei_neu@outlook.com">Email</a> &nbsp/&nbsp
                <a href="data/cv.pdf">Resume</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=wzbJ5EIAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/libeineu/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/LiBei.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/libei.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Research</heading>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
                I major in natural language processing, especially the sequence generation task, including machine translation, abstractive summarization and etc.
                My current focus majorly is to build parameter-efficient backbone for NLP. 

              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>News</heading>
            <ul>
              <li>
                [May'2022] Started my internship at MicroSoft Research Asia<a href="https://www.msra.cn/">(MSRA)</a>.
              </li>
               <li>
                [Apr'2022] <strong>One</strong> paper on learning multiscale Transformer models for sequence generation accepted by <a href="https://icml.cc/Conferences/2022">ICML 2022</a>.
              </li>
              <li>
                [Feb'2022] <strong>Two</strong> papers on parameter-efficient backbone and multimodal machine translation accepted by <a href="https://www.2022.aclweb.org/">ACL 2022</a>.
              </li> 
              <li>
                [Apr'2021] <strong>One</strong> paper on knowledge distillation accepted to <a href="https://2021.aclweb.org/">ACL 2021</a>.
              </li>
              <li>
                [Nov'2020] <strong>One</strong> paper on deep Transformer compression accepted to <a href="https://aaai.org/Conferences/AAAI-21/">AAAI 2021</a>.
              </li>
              <li>
                [Sep'2020] <strong>One</strong> paper on shallow-to-deep training for deep Transformer models accepted to <a href="https://2020.emnlp.org/">EMNLP 2020</a>.
              </li>
              <li>
                [Apr'2020] <strong>One</strong> paper on context-aware machine translation accepted to <a href="https://2020.aclweb.org/">ACL 2020</a>.
              </li>
              <li>
                [May'2019] <strong>One</strong> paper on the NiuTrans submission of WMT19 accepted to <a href="https://www.statmt.org/wmt19/">WMT 2019</a>.
              </li>
              <li>
                [May'2019] <strong>One</strong> paper on learning deep Transformer models accepted to <a href="https://2019.aclweb.org/">ACL 2019</a>.
              </li>
              

            </ul>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Publications</heading>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/umst.png' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Learning Multiscale Transformer Models for Sequence Generation</papertitle>
              <!-- </a> -->
              <br>
              <strong>Bei Li</strong>,
              Tong Zheng, Yi Jing, Chengbo Jiao, Tong Xiaoand Jingbo Zhu
              <br>
              <em>International Conference on Machine Learning(<strong>ICML, Spotlight</strong>)</em>, 2022  
              <br>
							<!-- <a href="#">[pdf](not aviable)</a> / 
							<a href="#">[code](not aviable)</a>  -->
							<a href="https://proceedings.mlr.press/v162/li22ac/li22ac.pdf">[pdf]</a> / [<a href="https://github.com/libeineu/UMST">code</a>]
              <p></p>
              <p>We re-define the concept of scale for NLP, including scales of sub-word, word and phrase. Our intention is to leverage the word boundaries and phrase-level prior knowledge to compensate for the sub-word features. Then we establish the relationships among different scales, resulting in builting a multiscale Transformer model.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/multimodal.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>On Vision Features in Multimodal Machine Translation</papertitle>
              <!-- </a> -->
              <br>
              <strong>Bei Li</strong>,
              Chuanhao Lv, Zefan Zhou, Tao Zhou, Tong Xiao, Anxiang Ma and Jingbo Zhu
              <br>
              <em>60th Annual Meeting of the Association for Computational Linguistics(<strong>ACL</strong>)</em>, 2022  
              <br>
							<a href="https://aclanthology.org/2022.acl-long.438.pdf">[pdf]</a> / [<a href="https://github.com/libeineu/fairseq_mmt">code</a>]
							<!-- <a href="#">[code](not aviable)</a>  -->
              <p></p>
              <p> This work investigates the effect of vision features in multimodal machine translation (MMT) scenarios. We proposed three probing tasks to evaluate MMT systems which can help the following researchers. The main contribution is to reveal the importance of strong vision features.</p>
            </td>
          </tr>
				
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/ode_transformer.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation
                  </papertitle>
              <!-- </a> -->
              <br> 
              <strong>Bei Li</strong>,
              Quan Du, Tao Zhou, Yi Jing, Shuhan Zhou, Xin Zeng, Tong Xiao,and Jingbo Zhu
              <br>
              <em>60th Annual Meeting of the Association for Computational Linguistics(<strong>ACL</strong>)</em>, 2022  
              <br> 
              [<a href="https://aclanthology.org/2022.acl-long.571.pdf">pdf</a>] / [<a href="https://github.com/libeineu/ODE-Transformer">code</a>]
              <p></p>
              <p>This work attempts to further enhance the standard sequence-level KD method by taking full advantage of the teacher parameters and generate the parameters for student.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/acl2021.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Weight Distillation: Transferring the Knowledge in Neural Network Parameters
                  </papertitle>
              <!-- </a> -->
              <br> 
              Ye Lin, Yanyang Li, Ziyang Wang, <strong>Bei Li</strong>, Quan Du, Tong Xiao, Jingbo Zhu
              <br>
              <em>59th Annual Meeting of the Association for Computational Linguistics(<strong>ACL, Oral</strong>)</em>, 2021  
              <br> 
              [<a href="https://aclanthology.org/2021.acl-long.162.pdf">pdf</a>] / [code]
              <p></p>
              <p>This work establishes the relationship between ODE and the design of Transformer architecture. We also redesign the Transformer architecture inspired by the lower truncation error achieved by high-order solvers in ODE. ODE Transformer can deliver much better translation performance within the same model capacity. Experimental results on three sequence generation tasks demonstrate the effectiveness.
              </p>
            </td>
          </tr>
 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/GPKD.png' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Learning Light-Weight Translation Models from Deep Transformer
                  </papertitle>
              <!-- </a> -->
              <br> 
              <strong>Bei Li</strong>,
              Ziyang Wang, Hui Liu, Quan Du, Tong Xiao, Chunliang Zhang, Jingbo Zhu
              <br>
              <em>Thirty-Fifth AAAI Conference on Artificial Intelligence(<strong>AAAI</strong>)</em>, 2021  
              <br> 
              [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/17561">pdf</a>] / [<a href="https://github.com/libeineu/GPKD">code</a>]
              <p></p>
              <p>This work attempts to learn a light-weight translation model from a deep Transformer teacher network. It introduces a group-permutation based knowledge distillation method to compressing a strong deep Transformer teacher into a much shallower counterpart with a minor BLEU degradation. Furthermore, to enhance the performance of the teacher network, we also propose a skipping sub-layer regularization training method to randomly omit some sub-layers vertically. Both methods can be well applicable into the teacher training process.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/sdt.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Shallow-to-Deep Training for Neural Machine Translation
                  </papertitle>
              <!-- </a> -->
              <br> 
              <strong>Bei Li</strong>,
              Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du, Tong Xiao, Huizhen Wang, Jingbo Zhu
              <br>
              <em>The 2020 Conference on Empirical Methods in Natural Language Processing(<strong>EMNLP</strong>)</em>, 2020  
              <br> 
              [<a href="https://aclanthology.org/2021.acl-long.162.pdf">pdf</a>] / [<a href="https://github.com/libeineu/SDT-Training">code</a>]
              <p></p>
              <p>Deep Transformer systems have been widely investigated in the MT community recently. However, with the model going deeper, a crucial challenge is the huge memory cost and extremely long training time. We investigate the behavior of trained systems and find that adjacent layers behave similarly. Thus, we proposed a shallow-to-deep training method instead of learning from scratch which speeds up the training process up to 1.5 times with no loss in BLEU.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/context-aware.png' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation
                  </papertitle>
              <!-- </a> -->
              <br> 
              <strong>Bei Li</strong>,
              Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li
              <br>
              <em>58th Annual Meeting of the Association for Computational Linguistics(<strong>ACL</strong>)</em>, 2020  
              <br> 
              [<a href="https://aclanthology.org/2021.acl-long.162.pdf">pdf</a>] / [<a href="https://github.com/libeineu/Context-Aware">code</a>]
              <p></p>
              <p>We investigate a general-used multi-encoder framework on document-level machine translation task. It utilizes an additional context-encoder to capture the relationship between the current sentence and its contextual information. However, through specially designed context inputs, we find that the context-encoder acts more like a noise generator instead of encoding the contextual information, which is similar with dropout.Especially when we turn off the context-encoder during inference, there is even slight improvements in terms of BLEU score.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/dlcl.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Learning deep transformer models for machine translation
                  </papertitle>
              <!-- </a> -->
              <br> 
              Qiang Wang,
              <strong>Bei Li</strong>,
              Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, Lidia S Chao 
              <br>
              <em>57th Annual Meeting of the Association for Computational Linguistics(<strong>ACL, Oral</strong>)</em>, 2019  
              <br> 
              [<a href="https://aclanthology.org/P19-1176.pdf">pdf</a>] / [<a href="https://github.com/wangqiangneu/dlcl">code</a>]
              <p></p>
              <p>It studies deep encoders in Transformer and mathematically explains the importance of the location of layer normalization for deep models. It also proposes a novel connection schema to successfully train a 30-layer Transformer system, which is the deepest encoder at that time. While, it is one of the most high cited NMT papers.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/wmt19.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>The niutrans machine translation systems for wmt19
                  </papertitle>
              <!-- </a> -->
              <br>
              <strong>Bei Li</strong>,
              Yinqiao Li, Chen Xu, Ye Lin, Jiqiang Liu, Hui Liu, Ziyang Wang, Yuhao Zhang, Nuo Xu, Zeyang Wang, Kai Feng, Hexuan Chen, Tengbo Liu, Yanyang Li, Qiang Wang, Tong Xiao, Jingbo Zhu
              <br>
              <em>Fourth Conference on Machine Translation(<strong>WMT, Workshop of ACL</strong>)</em>, 2019 
              <br>
							<a href="https://aclanthology.org/W19-5325.pdf">[pdf]</a> / 
							<!-- <a href="#">[code](not aviable)</a>  -->
              [code]
              <p></p>
              <p>It describes the submission of the NiuTrans systems for WMT2019 on both supervised and unsupervised tasks, including 13 language directions. This paper shows the details about model architectures, data augmentation methods, ensemble knowledge distillation and system combination strategies.
              </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Honors & Awards</heading>
            <ul style="list-style-type:none">
              <p style="line-height: 120%; margin-left: 0px; margin-top: 8pt; margin-bottom: 8pt;"> <font face="Arial" size="3">

              <li style="margin-bottom:5px">
                Top Ten Graduate students of Northeastern University (The May 4th medal). <div style="float:right; text-align:right">2022</div>
              </li>

              <li style="margin-bottom:5px">
                National Scholarship (PH.D). <div style="float:right; text-align:right">2021</div>
              </li>

              <li style="margin-bottom:5px">
                Outstanding Reviewers of EMNLP2021. <div style="float:right; text-align:right">2021</div>
              </li>

              <li style="margin-bottom:5px">
                1st Rank in Chinese-English in terms of human-evaluation on WMT21. <div style="float:right; text-align:right">2021</div>
              </li>

              <li style="margin-bottom:5px">
                The Excellent Master thesis of Liaoning Province. <div style="float:right; text-align:right">2020</div>
              </li>

              <li style="margin-bottom:5px">
                The Excellent Master Graduate of Liao Ning Province. <div style="float:right; text-align:right">2020</div>
              </li>

              <li style="margin-bottom:5px">
                The Excellent Master Graduate of Northeastern. <div style="float:right; text-align:right">2020</div>
              </li>

              <li style="margin-bottom:5px">
                1st Rank in Japanese-English news translation in terms of human-evaluation on WMT20. <div style="float:right; text-align:right">2020</div>
              </li>
              
              <li style="margin-bottom:5px">
                National Scholarship (Master). <div style="float:right; text-align:right">2019</div>
              </li>

              <li style="margin-bottom:5px">
                1st Rank in 3 news translation in terms of auto-evaluation on WMT19. <div style="float:right; text-align:right">2019</div>
              </li>

              <li style="margin-bottom:5px">
                2nd Rank in 3 news translation in terms of auto-evaluation on WMT19. <div style="float:right; text-align:right">2019</div>
              </li>

              <li style="margin-bottom:5px">
                National Scholarship (Master, Rank 1/230). <div style="float:right; text-align:right">2018</div>
              </li>
              
              <li style="margin-bottom:5px">
                The Excellent Graduate of Shenyang. <div style="float:right; text-align:right">2018</div>
              </li>
              
              <li style="margin-bottom:5px">
                1st Rank in Chinese-English news translation in terms of human-evaluation on WMT18. <div style="float:right; text-align:right">2018</div>
              </li>
              <li style="margin-bottom:5px">
                2nd Rank in English-Chinese news translation in terms of auto-evaluation on WMT18. <div style="float:right; text-align:right">2018</div>
              </li>


              <!-- <li style="margin-bottom:5px"><a href="https://challenge.ai.mgtv.com/home">MangoTV</a> International audio and video algorithm competition <strong> (Rank 6/193)</strong>. <div style="float:right; text-align:right">2021</div> -->
              </li>
              <br>
              </p>
              </ul>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <heading>Intern Experiences</heading>
          <tr>
            <td  style="padding:20px;width:35%;vertical-align:middle">
              <img width="160" src="./images/weiruan.png">
            </td>
            <td style="margin-left:20px;width:65%;vertical-align:middle">
              <div >
                Research Intern, MicroSoft Research Asia, Natural Language Computing
              </div>
              Mar. 2022 - Now <br>
              Text-to-Image Generation, Diffusion Models, Multimodal Modeling
            </td>
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Professional activities</heading>
            <ul style="list-style-type:none">
              <p style="line-height: 120%; margin-left: 0px; margin-top: 8pt; margin-bottom: 8pt;"> <font face="Arial" size="3">
              <li >
                Conference Reviewer for ACL, EMNLP, ICML, Neurips, AAAI, IJCAI, NAACL, COLING, EACL
              </li>
              <br>
              </p>
              </ul>

        </tbody></table>

				
     
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
             
                <p> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &copy; Bei Li | Last updated: Oct. 2022.</p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
